{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica Final\n",
    "Modulo 6 - Máster Data Science y Business Analytics\n",
    "Sergio Hervás Aragón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import split, to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al trabajar con pyspark, configuraremos el entorno creando una sesion de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# master(String master): Establece la dirección URL maestra de Spark a la que se va a conectar\n",
    "# appName(String name): Establece un nombre para la aplicación, que se mostrará en la interfaz de usuario web de Spark.\n",
    "# config(String key, double value): Establece una opción de configuración.\n",
    "# getOrCreate(): Obtiene una SparkSession existente o, si no hay ninguna, crea una nueva uno basado en las opciones establecidas en este constructor.\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master('local')\\\n",
    "        .appName('netflix_titles')\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\\\n",
    "        .getOrCreate()\n",
    "except Exception as e:\n",
    "    print(f'Ha ocurrido un error: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tLeer todos los csv descomprimidos guardados en la ruta de vuestro tmp en una sola línea de código (pista, usar wildcards para leer más de un fichero a la vez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, procederemos a descarganos los archivos mediante una linea de comandos linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaro una lista con los archivos de los cuales voy ha hacer uso y me voy a descargar\n",
    "csv_files = [\n",
    "    '/netflix_titles_dirty_01.csv',\n",
    "    '/netflix_titles_dirty_02.csv',\n",
    "    '/netflix_titles_dirty_03.csv',\n",
    "    '/netflix_titles_dirty_04.csv',\n",
    "    '/netflix_titles_dirty_05.csv',\n",
    "    '/netflix_titles_dirty_06.csv',\n",
    "    '/netflix_titles_dirty_07.csv'\n",
    "]\n",
    "\n",
    "# Variable la cual usaremos en un futuro para indicar donde almacenar los archivos de la lista\n",
    "first_ending = '/tmp'\n",
    "# Variable la cual usaremos en un futuro para indicar la manera en la que nos vamos a descargar los archivos de la lista\n",
    "ultimate_termination = '.gz'\n",
    "# Bucle con el que recorreremos la lista\n",
    "for file in csv_files:    \n",
    "    # Declararemos dos variables que nos serviran para validar, si: 1º - Esta descargado el archivo, y 2º - Si el archivo esta descomprimido\n",
    "    compressed_path = first_ending + file + ultimate_termination\n",
    "    decompressed_path = first_ending + file    \n",
    "    # Buscamos el path, y si no existe tanto el comprimido como el descomprimido, lo descargaremos y los descomprimiremos\n",
    "    if not os.path.exists(compressed_path and decompressed_path):\n",
    "        url = f'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data{file}.gz?raw=True'\n",
    "        try:\n",
    "            # Descargamos\n",
    "            ! wget -O $compressed_path $url\n",
    "            # Descomprimimos\n",
    "            ! gunzip $compressed_path\n",
    "        except Exception as e:\n",
    "            print(f'Ha ocurrido un error: {str(e)}')\n",
    "        finally:\n",
    "            print(f'Archivo {file} descargado y descomprimido ')\n",
    "    else:\n",
    "        print(f'El fichero {file} ya existe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('./../../../tmp/*csv', sep='\\t', header=False)\n",
    "df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tAnaliza las columnas y renómbralos con un nombre que tenga sentido para cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaro una lista que contendran los nombres de las columnas nuevas las cuales van a ser las que van a dar nombre a cada columna\n",
    "columsNames = ['id','type','movie_name','director','actors','country','release_date','year','age_classification','duration','gender','description']\n",
    "\n",
    "for item in range(len(columsNames)):\n",
    "    # Reemplazo con el bucle, en primera posicion, el nombre de la columba original, y en segunda posicion el nuevo nombre\n",
    "    df = df.withColumnRenamed(f'_c{item}', columsNames[item])\n",
    "    \n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tLimpia el dataframe para que no existan nulos, adicionalmente elimina todos los valores que no se correspondan con el resto de datos de la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaremos un metodo que nos servira para ver cuantos nulos hay por columnas en nuestro df\n",
    "def counts_off_nulls_spark(columsNames, df):\n",
    "    print('Conteo de nulos por columnas:')\n",
    "    for columns_name in columsNames:\n",
    "        # Recorro el array de mis columnas filtrando por nombre de las columnas los valores que son nulos, y haciendo un conteo de estos\n",
    "        number_nulls_columns = df.filter(df[columns_name].isNull()).count()\n",
    "        print(f'\\t{columns_name}: {number_nulls_columns} nulls number')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que nos realiza un conteo de nulos antes de eliminarlos\n",
    "counts_off_nulls_spark(columsNames,df)\n",
    "# Eliminamos los nulos\n",
    "df = df.dropna()\n",
    "# Funcion que nos realiza un conteo de nulos despues de eliminarlos\n",
    "counts_off_nulls_spark(columsNames,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tRevisa el tipo de dato de cada columna y parsealo según corresponda (la columna duración debe ser numérico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos primeramente un arreglo de la columna duracion, donde separaremos el contenido mediente el espacio formando dos\n",
    "# columnas con la respectiva información, y elimaremos la columna original\n",
    "if 'duration' in df.columns:\n",
    "    df = df.\\\n",
    "        withColumn('time_duration', split(df['duration'], ' ')[0]).\\\n",
    "        withColumn('type_duration', split(df['duration'], ' ')[1]).\\\n",
    "        drop('duration')\n",
    "    \n",
    "# Una vez todas las columnas como queremos, procedemos al parseo de la información, emprezando por los enteros (id, year, time_duration)\n",
    "df = df.\\\n",
    "    withColumn('id', df['id'].cast('int')).\\\n",
    "    withColumn('year', df['year'].cast('int')).\\\n",
    "    withColumn('time_duration', df['time_duration'].cast('int'))\n",
    "    \n",
    "# Seguidamente procederemos al parseo de la información en tipo Date (release_date)\n",
    "df = df.withColumn(\"release_date\", to_date(df['release_date'], 'MMMM dd, yyyy'))\n",
    "    \n",
    "df.printSchema()\n",
    "df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tCalcula la duración media en función del país"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tAnaliza las columnas y renómbralos con un nombre que tenga sentido para cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------------------+-----------+------------+-------------+-------------+----+------------------+--------+---------------+--------------------+\n",
      "|      id| type|         movie_name|   director|      actors|      country| release_date|year|age_classification|duration|         gender|         description|\n",
      "+--------+-----+-------------------+-----------+------------+-------------+-------------+----+------------------+--------+---------------+--------------------+\n",
      "|80044126|Movie|D.L. Hughley: Clear|Jay Chapman|D.L. Hughley|United States|July 13, 2017|2014|             TV-MA|  59 min|Stand-Up Comedy|In this 2014 stan...|\n",
      "+--------+-----+-------------------+-----------+------------+-------------+-------------+----+------------------+--------+---------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Declaro una lista que contendran los nombres de las columnas nuevas las cuales van a ser las que van a dar nombre a cada columna\n",
    "columsNames = ['id','type','movie_name','director','actors','country','release_date','year','age_classification','duration','gender','description']\n",
    "\n",
    "for item in range(len(columsNames)):\n",
    "    # Reemplazo con el bucle, en primera posicion, el nombre de la columba original, y en segunda posicion el nuevo nombre\n",
    "    df = df.withColumnRenamed(f'_c{item}', columsNames[item])\n",
    "    \n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tLimpia el dataframe para que no existan nulos, adicionalmente elimina todos los valores que no se correspondan con el resto de datos de la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaremos un metodo que nos servira para ver cuantos nulos hay por columnas en nuestro df\n",
    "def counts_off_nulls_spark(columsNames, df):\n",
    "    print('Conteo de nulos por columnas:')\n",
    "    for columns_name in columsNames:\n",
    "        # Recorro el array de mis columnas filtrando por nombre de las columnas los valores que son nulos, y haciendo un conteo de estos\n",
    "        number_nulls_columns = df.filter(df[columns_name].isNull()).count()\n",
    "        print(f'\\t{columns_name}: {number_nulls_columns} nulls number')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de nulos por columnas:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tid: 2 nulls number\n",
      "\ttype: 59 nulls number\n",
      "\tmovie_name: 64 nulls number\n",
      "\tdirector: 2012 nulls number\n",
      "\tactors: 629 nulls number\n",
      "\tcountry: 537 nulls number\n",
      "\trelease_date: 80 nulls number\n",
      "\tyear: 74 nulls number\n",
      "\tage_classification: 84 nulls number\n",
      "\tduration: 76 nulls number\n",
      "\tgender: 83 nulls number\n",
      "\tdescription: 86 nulls number\n",
      "Conteo de nulos por columnas:\n",
      "\tid: 0 nulls number\n",
      "\ttype: 0 nulls number\n",
      "\tmovie_name: 0 nulls number\n",
      "\tdirector: 0 nulls number\n",
      "\tactors: 0 nulls number\n",
      "\tcountry: 0 nulls number\n",
      "\trelease_date: 0 nulls number\n",
      "\tyear: 0 nulls number\n",
      "\tage_classification: 0 nulls number\n",
      "\tduration: 0 nulls number\n",
      "\tgender: 0 nulls number\n",
      "\tdescription: 0 nulls number\n"
     ]
    }
   ],
   "source": [
    "# Funcion que nos realiza un conteo de nulos antes de eliminarlos\n",
    "counts_off_nulls_spark(columsNames,df)\n",
    "# Eliminamos los nulos\n",
    "df = df.dropna()\n",
    "# Funcion que nos realiza un conteo de nulos despues de eliminarlos\n",
    "counts_off_nulls_spark(columsNames,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tRevisa el tipo de dato de cada columna y parsealo según corresponda (la columna duración debe ser numérico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parseo de fecha finalizado\n",
      "Parseo de enteros finalizado\n",
      "Parseo de fechas finalizado\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- movie_name: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- actors: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- release_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- age_classification: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- time_duration: integer (nullable = true)\n",
      " |-- type_duration: string (nullable = true)\n",
      "\n",
      "+--------+-----+-------------------+-----------+------------+-------------+------------+----+------------------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------------+\n",
      "|id      |type |movie_name         |director   |actors      |country      |release_date|year|age_classification|gender         |description                                                                                                                                      |time_duration|type_duration|\n",
      "+--------+-----+-------------------+-----------+------------+-------------+------------+----+------------------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------------+\n",
      "|80044126|Movie|D.L. Hughley: Clear|Jay Chapman|D.L. Hughley|United States|2017-07-13  |2014|TV-MA             |Stand-Up Comedy|In this 2014 standup special filmed in San Francisco, comedic genius D.L. Hughley entertains with his hilarious take on current affairs and more.|59           |min          |\n",
      "+--------+-----+-------------------+-----------+------------+-------------+------------+----+------------------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Realizamos primeramente un arreglo de la columna duracion, donde separaremos el contenido mediente el espacio formando dos\n",
    "    # columnas con la respectiva información, y elimaremos la columna original\n",
    "    if 'duration' in df.columns:\n",
    "        df = df.\\\n",
    "            withColumn('time_duration', split(df['duration'], ' ')[0]).\\\n",
    "            withColumn('type_duration', split(df['duration'], ' ')[1]).\\\n",
    "            drop('duration')\n",
    "except Exception as e:\n",
    "    print(f'Ha ocurrido un error: {str(e)}')\n",
    "finally:\n",
    "    print(f'Parseo de fecha finalizado')\n",
    "    \n",
    "try:\n",
    "# Una vez todas las columnas como queremos, procedemos al parseo de la información, emprezando por los enteros (id, year, time_duration)\n",
    "    df = df.\\\n",
    "        withColumn('id', df['id'].cast('int')).\\\n",
    "        withColumn('year', df['year'].cast('int')).\\\n",
    "        withColumn('time_duration', df['time_duration'].cast('int'))\n",
    "except Exception as e:\n",
    "    print(f'Ha ocurrido un error: {str(e)}')\n",
    "finally:\n",
    "    print(f'Parseo de enteros finalizado')\n",
    "    \n",
    "try:\n",
    "    # Seguidamente procederemos al parseo de la información en tipo Date (release_date)\n",
    "    df = df.withColumn(\"release_date\", to_date(df['release_date'], 'MMMM dd, yyyy'))\n",
    "except Exception as e:\n",
    "    print(f'Ha ocurrido un error: {str(e)}')\n",
    "finally:\n",
    "    print(f'Parseo de fechas finalizado')\n",
    "    \n",
    "df.printSchema()\n",
    "df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tCalcula la duración media en función del país"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía \n",
    "\n",
    " - [Como hacer hipervinculos](https://learn.microsoft.com/es-es/contribute/content/how-to-write-links)\n",
    "\n",
    "#### Apartado 1:\n",
    "\n",
    " - [¿Que son los wildcards?](https://support.microsoft.com/en-us/office/examples-of-wildcard-characters-939e153f-bd30-47e4-a763-61897c87b3f4#:~:text=Wildcards%20are%20special%20characters%20that,named%20John%20on%20Park%20Street.)\n",
    "\n",
    " - [Visualización de la ejecución de mi aplicación netflix_titles en modo local (http://localhost:4050/)](http://localhost:4050/)\n",
    "\n",
    " - [Información del objeto builder](https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/sql/SparkSession.Builder.html)\n",
    "\n",
    " - [Expansión del patrón de nombres de ruta de estilo Unix (glob)](https://docs.python.org/es/3/library/glob.html)\n",
    "\n",
    " - [path](https://www.guru99.com/es/python-check-if-file-exists.html)\n",
    "\n",
    " - [Ejecutar comandos de shell en Jupyter Notebook](https://blogs.upm.es/estudiaciencia/variables-en-bash/)\n",
    "\n",
    " #### Apartado 2:\n",
    "\n",
    " - [Cambiar el nombre de columnas usando 'withColumnRenamed'](https://www.machinelearningplus.com/pyspark/pyspark-rename-columns/?utm_content=cmp-true)\n",
    "\n",
    " #### Apartado 3:\n",
    "\n",
    " - [pyspark.sql.DataFrame.printSchema](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.printSchema.html)\n",
    "\n",
    " - [truncate (referencia apartado 11)](https://stackoverflow.com/questions/33742895/how-to-show-full-column-content-in-a-spark-dataframe)\n",
    "\n",
    " #### Apartado 4:\n",
    "\n",
    " - [Cast Column Type With Example](https://sparkbyexamples.com/pyspark/pyspark-cast-column-type/)\n",
    "\n",
    " - [Spark – Split DataFrame single column into multiple columns](https://sparkbyexamples.com/spark/spark-split-dataframe-column-into-multiple-columns/)\n",
    "\n",
    " - [Ver etiquetas de columnas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html)\n",
    "\n",
    " - [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER](https://community.databricks.com/t5/data-engineering/inconsistent-behavior-cross-version-parse-datetime-by-new-parser/td-p/43674)\n",
    "\n",
    " #### Apartado 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
